
# Application Streamlit unique : Incidence des maladies (C√¥te d'Ivoire, 2012-2015)

# -------- Imports (tous comment√©s) --------
from pathlib import Path                                # Pour g√©rer des chemins d'assets robustement
import streamlit as st                                  # Pour construire l'application web
import pandas as pd                                     # Pour manipuler les donn√©es tabulaires
import numpy as np                                      # Pour op√©rations num√©riques
import plotly.express as px                             # Pour graphiques interactifs
from streamlit.components.v1 import html as html_component  # Composant html si fallback Pygwalker
from io import StringIO                                 # Pour fabriquer un CSV t√©l√©chargeable en m√©moire
from sklearn.model_selection import train_test_split, cross_val_score  # Split & validation crois√©e
from sklearn.compose import ColumnTransformer           # Pipeline de pr√©traitement par type de colonnes
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures  # Encodage/standardisation/features
from sklearn.pipeline import Pipeline                   # Pour cha√Æner pr√©traitement + mod√®le
from sklearn.linear_model import LinearRegression       # R√©gression lin√©aire
from sklearn.ensemble import RandomForestRegressor      # For√™t al√©atoire (r√©gression)
from sklearn.neighbors import KNeighborsRegressor       # KNN r√©gression
from sklearn.neural_network import MLPRegressor        # R√©seau de neurones l√©ger
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error  # M√©triques
from pandas.api import types as ptypes                  # Outils de d√©tection de dtypes pandas

# -------- Pr√©f√©rences pandas (√©viter avertissements futurs inutiles) --------
pd.set_option("future.no_silent_downcasting", True)     # √âvitons les downcasts silencieux d√©pr√©ci√©s (messages vus dans logs)

# -------- Utilitaires d‚Äôassets --------
def chemin_asset(relatif: str) -> Path:
    """Construisons un chemin absolu s√ªr vers un asset du d√©p√¥t."""
    # Utilisons le r√©pertoire courant de l‚Äôapp (l√† o√π se trouve app.py) pour r√©soudre la ressource.
    return (Path(__file__).parent / relatif).resolve()

def image_page_icon() -> str:
    """Chargeons une ic√¥ne de page si disponible dans assets/, sinon utilisons un emoji."""
    p = chemin_asset("assets/moustique_tigre.jpg")  # Chargeons le fichier assets/moustique_tigre.jpg pour l‚Äôic√¥ne
    if p.exists():
        return str(p)                               # Servons le chemin si l‚Äôimage est pr√©sente
    return "ü¶ü"                                      # Fallback emoji si l‚Äôimage n‚Äôest pas disponible

# -------- Configuration globale de la page --------
st.set_page_config(                                     # Configurons la page Streamlit pour un rendu propre
    page_title="Incidence maladies CI (2012-2015)",     # D√©finissons le titre de l‚Äôonglet navigateur
    page_icon=image_page_icon(),                        # D√©finissons l‚Äôic√¥ne de page (image assets/ si dispo, sinon emoji)
    layout="wide"                                       # Passons en mode large pour mieux exploiter l‚Äô√©cran
)

# -------- Styles CSS l√©gers pour homog√©n√©iser l'UI --------
st.markdown("""
<style>
.block { background: #ffffff; padding: 16px; border-radius: 12px;
         box-shadow: 0 2px 10px rgba(0,0,0,0.06); }
h2, h3 { color: #0D1D2C; }
[data-testid="stDataFrame"] { border: 1px solid #eee; border-radius: 8px; }
</style>
""", unsafe_allow_html=True)

# --------- Fonctions utilitaires (toutes en fran√ßais + pourquoi) ---------

@st.cache_data(show_spinner=True, ttl=3600)
def charger_donnees_csv(chemin: str) -> pd.DataFrame:
    """Chargeons le fichier CSV pour alimenter l‚Äôapplication (source unique) afin d‚Äô√©viter des IO r√©p√©t√©es."""
    # Chargeons le fichier "Incidence.csv" pour servir toutes les pages
    df = pd.read_csv(chemin)  # Lecture simple du CSV partag√©
    return df                 # Renvoyons le DataFrame charg√©

def normaliser_noms_colonnes(df: pd.DataFrame) -> pd.DataFrame:
    """Harmonisons les libell√©s de colonnes pour un code robuste (variantes orthographiques incluses)."""
    donnees = df.copy()  # Copions le DataFrame d‚Äôentr√©e pour travailler en s√©curit√©

    # Correspondance libell√©s h√©t√©rog√®nes -> noms normalis√©s
    mapping = {
        "ANNEE": "annee",
        "REGIONS / DISTRICTS": "regions_districts",
        "REGIONS/DISTRICTS": "regions_districts",
        "VILLES / COMMUNES": "villes_communes",
        "VILLES/COMMUNES": "villes_communes",
        "MALADIE": "maladie",
        "INCIDENCE SUR LA POPULATION GENERALE (%)": "incidence_population_pct",
        "INCIDENCE_SUR_LA_POPULATION_GENERALE_(%)": "incidence_population_pct",
    }
    # Renommons ce qui est pr√©sent
    colonnes_renommees = {c: mapping[c] for c in donnees.columns if c in mapping}
    donnees = donnees.rename(columns=colonnes_renommees)

    # Normalisons tout le reste (minuscules + underscore)
    donnees.columns = [c.strip().lower().replace(" ", "_") for c in donnees.columns]
    return donnees

def typer_colonnes(donnees: pd.DataFrame) -> pd.DataFrame:
    """Typage : convertissons ann√©e en entier tol√©rant NA, incidence en float, autres en string pour coh√©rence."""
    df = donnees.copy()
    if "annee" in df.columns:
        # Convertissons en entier ‚Äúnullable‚Äù au d√©part (puis conversion Arrow-compat plus bas)
        df["annee"] = pd.to_numeric(df["annee"], errors="coerce").astype("Int64")
    if "incidence_population_pct" in df.columns:
        df["incidence_population_pct"] = pd.to_numeric(df["incidence_population_pct"], errors="coerce")
    for col in ["regions_districts", "villes_communes", "maladie"]:
        if col in df.columns:
            df[col] = df[col].astype("string")
    return df

def statistiques_rapides(df: pd.DataFrame) -> pd.DataFrame:
    """Produisons un tableau synth√©tique des statistiques num√©riques pour un survol rapide."""
    stats = df.select_dtypes(include=[np.number]).describe().T
    return stats

def valeurs_manquantes(df: pd.DataFrame) -> pd.DataFrame:
    """Comptons les valeurs manquantes par colonne pour cibler le nettoyage."""
    na = df.isna().sum().sort_values(ascending=False)
    return pd.DataFrame({"colonne": na.index, "manquants": na.values})

def nettoyer_donnees(df: pd.DataFrame) -> pd.DataFrame:
    """Nettoyons : supprimons les doublons, imputons NA (m√©diane num., mode cat√©goriel) pour fiabiliser les analyses."""
    donnees = df.copy()
    donnees = donnees.drop_duplicates()

    cols_num = donnees.select_dtypes(include=[np.number, "Int64", "Float64"]).columns.tolist()
    cols_cat = [c for c in donnees.columns if c not in cols_num]

    for c in cols_num:
        if donnees[c].isna().any():
            donnees[c] = donnees[c].fillna(donnees[c].median())

    for c in cols_cat:
        if donnees[c].isna().any():
            mode = donnees[c].mode(dropna=True)
            if len(mode) > 0:
                donnees[c] = donnees[c].fillna(mode.iloc[0])

    return donnees

def rendre_arrow_compatible(df: pd.DataFrame) -> pd.DataFrame:
    """Convertissons les dtypes 'difficiles' (Int64 nullable, object mixte) pour st.dataframe/Arrow et √©viter les correctifs auto."""
    dfa = df.copy()

    for col in dfa.columns:
        dtype = dfa[col].dtype

        # 1) Entiers "nullable" pandas (Int64, Int32, ‚Ä¶) -> float64 si pr√©sence de NA (pr√©serve NA), sinon int64
        if ptypes.is_extension_array_dtype(dtype) and str(dtype).startswith("Int"):
            if dfa[col].isna().any():
                dfa[col] = dfa[col].astype("float64")
            else:
                dfa[col] = dfa[col].astype("int64")

        # 2) Objets h√©t√©rog√®nes : tentons la conversion num√©rique globale, sinon string
        elif dtype == "object":
            conv = pd.to_numeric(dfa[col], errors="ignore")
            if conv.dtype != "object":
                dfa[col] = conv
            else:
                dfa[col] = dfa[col].astype("string")

    return dfa

def detecter_valeurs_aberrantes(df: pd.DataFrame, z: float = 3.0) -> pd.DataFrame:
    """D√©tectons les valeurs aberrantes (z-score > seuil) pour les colonnes num√©riques afin d‚Äôinvestiguer des anomalies."""
    dnum = df.select_dtypes(include=[np.number])
    if dnum.empty:
        return df.head(0)
    zscores = (dnum - dnum.mean()) / dnum.std(ddof=0)
    masque_out = (zscores.abs() > z).any(axis=1)
    return df.loc[masque_out]

def telecharger_csv(df: pd.DataFrame) -> bytes:
    """Pr√©parons un CSV t√©l√©chargeable (en m√©moire) pour permettre l‚Äôexport des donn√©es nettoy√©es."""
    tampon = StringIO()
    df.to_csv(tampon, index=False)
    return tampon.getvalue().encode("utf-8")

# --------- Chargement unique & pr√©paration initiale ---------
# Chargeons le fichier 'Incidence.csv' pour alimenter l'app, puis normalisons/typons/nettoyons
donnees_brutes = charger_donnees_csv("Incidence.csv")        # Chargeons le fichier CSV pour obtenir les donn√©es d'origine
donnees = normaliser_noms_colonnes(donnees_brutes)           # Harmonisons les libell√©s pour unifier le code
donnees = typer_colonnes(donnees)                            # Typage coh√©rent (entiers, float, string)
donnees_nettoyees = nettoyer_donnees(donnees)                # Appliquons un nettoyage simple et robuste

# Stockons dans la session pour r√©utiliser partout sans rechargement
if "donnees_nettoyees" not in st.session_state:
    st.session_state["donnees_nettoyees"] = donnees_nettoyees

# --------- Barre de navigation horizontale (onglets) ---------
onglets = st.tabs([
    "üè† Accueil", "üìí Informations", "üõ† Exploration", "üßπ Pr√©paration",
    "üîç Visualisations", "üëÄ Explorateur", "„ÄΩÔ∏è Mod√©lisation", "‚óª Pr√©diction", "üõñ Source"
])

# =========================
# üè† ACCUEIL
# =========================
with onglets[0]:
    # Affichons un en-t√™te agr√©able
    st.title("Incidence des maladies en C√¥te d‚ÄôIvoire (2012‚Äì2015)")  # Titre principal de l'application
    col1, col2 = st.columns([1, 2], gap="large")                     # D√©coupons en deux colonnes pour la mise en page

    with col1:
        # Chargeons l'image uniquement si elle existe (dans assets/) pour √©viter tout crash
        img_path = chemin_asset("assets/moustique_tigre.jpg")        # Chargeons le fichier assets/moustique_tigre.jpg pour l‚Äôillustration
        if img_path.exists():
            st.image(str(img_path), use_column_width=True, caption="Aedes albopictus (moustique tigre)")  # Affichons l'image si pr√©sente
        else:
            st.markdown("### ü¶ü")  # Fallback visuel minimal si l‚Äôimage n‚Äôest pas disponible

    with col2:
        st.markdown("""<div class="block" style="text-align:justify">
        <h3>Objectif de l‚Äôapplication</h3>
        Cette application interactive permet d‚Äôexplorer, de visualiser et de mod√©liser 
        l‚Äôincidence de plusieurs maladies en C√¥te d‚ÄôIvoire entre 2012 et 2015.
        Elle propose des graphiques interactifs, un explorateur visuel libre (Pygwalker),
        ainsi que plusieurs mod√®les pr√©dictifs (R√©gression, Random Forest, KNN, ANN).
        </div>""", unsafe_allow_html=True)

        st.markdown("""<div class="block" style="text-align:justify">
        <h3>Probl√®me adress√©</h3>
        Comment transformer des donn√©es brutes de sant√© publique en <b>indicateurs actionnables</b> 
        et en <b>pr√©dictions</b> fiables pour aider √† la d√©cision (priorisation des zones et des pathologies) ?
        </div>""", unsafe_allow_html=True)

        st.markdown("""<div class="block" style="text-align:justify">
        <h3>R√©sultats attendus</h3>
        Un <b>outil unique</b> permettant : 
        (1) l‚Äôanalyse rapide des tendances,
        (2) l‚Äôidentification des disparit√©s r√©gionales,
        (3) la pr√©diction de l‚Äôincidence selon ann√©e/r√©gion/ville/maladie.
        </div>""", unsafe_allow_html=True)

# =========================
# üìí INFORMATIONS
# =========================
with onglets[1]:
    st.header("Informations sur les donn√©es")                      # Pla√ßons un en-t√™te clair
    st.write("**Aper√ßu des premi√®res lignes (jeu de donn√©es d‚Äôorigine)**")  # Introduisons l‚Äôaper√ßu
    st.dataframe(rendre_arrow_compatible(donnees_brutes.head()), use_container_width=True)  # Affichons 5 premi√®res lignes

    st.write("**Libell√©s de colonnes normalis√©s (utilis√©s en interne)**")   # Expliquons les noms normalis√©s
    st.json({
        "annee": "Ann√©e d‚Äôobservation (2012‚Äì2015)",
        "regions_districts": "R√©gion ou district sanitaire",
        "villes_communes": "Ville ou commune",
        "maladie": "Type de pathologie",
        "incidence_population_pct": "Incidence sur la population g√©n√©rale (en %)"
    })

# =========================
# üõ† EXPLORATION
# =========================
with onglets[2]:
    st.header("Exploration des donn√©es")  # Titre de section

    # Tables empil√©es verticalement (les unes en dessous des autres), comme demand√©
    st.subheader("Dimensions & dtypes")
    st.write(f"**Nombre de lignes** : {donnees_nettoyees.shape[0]}")
    st.write(f"**Nombre de colonnes** : {donnees_nettoyees.shape[1]}")
    st.write("**Types de donn√©es**")
    st.dataframe(
        rendre_arrow_compatible(donnees_nettoyees.dtypes.to_frame("dtype")),
        use_container_width=True
    )

    st.subheader("Valeurs manquantes par colonne")
    st.dataframe(
        rendre_arrow_compatible(valeurs_manquantes(donnees_nettoyees)),
        use_container_width=True
    )

    st.subheader("Statistiques descriptives (num√©rique)")
    st.dataframe(
        rendre_arrow_compatible(statistiques_rapides(donnees_nettoyees)),
        use_container_width=True
    )

    st.subheader("Valeurs aberrantes potentielles (z-score > 3)")
    outliers = detecter_valeurs_aberrantes(donnees_nettoyees)  # D√©tectons des anomalies possibles
    if outliers.empty:
        st.info("Aucune ligne ne d√©passe le seuil de z-score s√©lectionn√© (3.0).")
    else:
        st.dataframe(rendre_arrow_compatible(outliers), use_container_width=True)

# =========================
# üßπ PR√âPARATION (manipulation)
# =========================
with onglets[3]:
    st.header("Pr√©paration et export des donn√©es")                      # Titre de section

    st.write("**Aper√ßu apr√®s nettoyage (doublons supprim√©s, NA imput√©s)**")  # Introduisons l‚Äôaper√ßu post-nettoyage
    st.dataframe(rendre_arrow_compatible(donnees_nettoyees.head(20)), use_container_width=True)  # 20 premi√®res lignes

    # Bouton de t√©l√©chargement du CSV nettoy√©
    st.download_button(                                                  # Cr√©ons un bouton pour r√©cup√©rer le CSV propre
        label="üì• T√©l√©charger les donn√©es nettoy√©es (CSV)",              # √âtiquette du bouton
        data=telecharger_csv(donnees_nettoyees),                         # Contenu du fichier en m√©moire
        file_name="incidence_nettoyee.csv",                              # Nom du fichier propos√©
        mime="text/csv"                                                  # Type MIME
    )

# =========================
# üîç VISUALISATIONS
# =========================
with onglets[4]:
    st.header("Visualisations interactives")                             # Titre de section
    dfv = donnees_nettoyees                                              # Alias local

    # ----- Contr√¥les d‚Äôentr√©e pour filtrer les graphiques -----
    colf1, colf2, colf3 = st.columns(3)                                  # Trois filtres c√¥te √† c√¥te

    with colf1:
        maladies_dispo = sorted(dfv["maladie"].dropna().unique().tolist())   # Liste des maladies
        choix_maladie = st.selectbox("Choisir une maladie", maladies_dispo, index=0)  # S√©lecteur maladie

    with colf2:
        annees_dispo = sorted(dfv["annee"].dropna().astype(int).unique().tolist())    # Liste des ann√©es
        choix_annees = st.multiselect("Filtrer par ann√©e", annees_dispo, default=annees_dispo) # S√©lecteur multi

    with colf3:
        regions_dispo = ["(Toutes)"] + sorted(dfv["regions_districts"].dropna().unique().tolist()) # R√©gions
        choix_region = st.selectbox("Filtrer par r√©gion/district", regions_dispo, index=0)         # S√©lecteur r√©gion

    # ----- Appliquons les filtres -----
    dff = dfv[dfv["maladie"] == choix_maladie]                                 # Filtre maladie
    dff = dff[dff["annee"].isin(choix_annees)]                                 # Filtre ann√©es
    if choix_region != "(Toutes)":                                             # Filtre r√©gion si choisi
        dff = dff[dff["regions_districts"] == choix_region]

    # ----- Graphique 1 : √©volution temporelle -----
    st.subheader("√âvolution de l‚Äôincidence (%) dans le temps")
    if len(dff) == 0:
        st.warning("Aucune donn√©e pour ce filtre.")
    else:
        evol = dff.groupby("annee", dropna=True)["incidence_population_pct"].mean().reset_index()  # Moyenne par ann√©e
        fig1 = px.line(evol, x="annee", y="incidence_population_pct",
                       markers=True, labels={"annee": "Ann√©e", "incidence_population_pct": "Incidence (%)"},
                       title=f"√âvolution ‚Äî {choix_maladie}")
        st.plotly_chart(fig1, use_container_width=True)

    # ----- Graphique 2 : comparaison par r√©gion (moyenne) -----
    st.subheader("Comparaison des r√©gions/districts (moyenne)")
    comp = dfv[dfv["maladie"] == choix_maladie]
    comp = comp[comp["annee"].isin(choix_annees)]
    comp = comp.groupby("regions_districts", dropna=True)["incidence_population_pct"].mean().reset_index()
    comp = comp.sort_values("incidence_population_pct", ascending=False).head(20)  # Top 20 pour lisibilit√©
    fig2 = px.bar(comp, x="regions_districts", y="incidence_population_pct",
                  labels={"regions_districts": "R√©gion/District", "incidence_population_pct": "Incidence (%)"},
                  title=f"Top r√©gions ‚Äî {choix_maladie} (moyenne {min(choix_annees)}‚Äì{max(choix_annees)})")
    fig2.update_layout(xaxis_tickangle=-45)  # Lisibilit√© des labels
    st.plotly_chart(fig2, use_container_width=True)

    # ----- Graphique 3 : r√©partition par ann√©e (camembert) -----
    st.subheader("R√©partition par ann√©e (moyenne)")
    rep = dff.groupby("annee", dropna=True)["incidence_population_pct"].mean().reset_index()
    fig3 = px.pie(rep, names="annee", values="incidence_population_pct",
                  title="Part relative moyenne par ann√©e", hole=0.35)
    st.plotly_chart(fig3, use_container_width=True)

# =========================
# üëÄ EXPLORATEUR (Pygwalker)
# =========================
with onglets[5]:
    st.header("Explorateur visuel libre (Pygwalker)")
    st.info("Astuce : glissez-d√©posez les champs √† gauche pour cr√©er vos vues interactives.")

    try:
        # Utilisons l'API Streamlit officielle de Pygwalker (plus stable que l'HTML brut)
        from pygwalker.api.streamlit import StreamlitRenderer

        @st.cache_resource(show_spinner=False)
        def obtenir_pyg_renderer(df: pd.DataFrame):
            """Construisons un renderer Pygwalker mis en cache pour de meilleures performances."""
            # Passons le DataFrame d√©j√† nettoy√© ; spec_io_mode="rw" pour permettre sauvegarde locale de spec
            return StreamlitRenderer(df, spec_io_mode="rw")

        renderer = obtenir_pyg_renderer(donnees_nettoyees)  # Instancions le renderer en ressource cach√©e
        renderer.explorer(height=900, scrolling=True, default_tab="vis")  # Affichons le studio

    except Exception as e:
        st.error("Pygwalker n‚Äôa pas pu √™tre charg√© (v√©rifiez les versions).")
        st.exception(e)
        # Fallback minimal (d√©sactiv√© par d√©faut) :
        # try:
        #     import pygwalker as pyg
        #     html = pyg.to_html(donnees_nettoyees)
        #     html_component(html, height=900, scrolling=True)
        # except Exception:
        #     pass

# =========================
# „ÄΩÔ∏è MOD√âLISATION
# =========================
with onglets[6]:
    st.header("Mod√©lisation supervis√©e (r√©gression)")

    # --------- Param√®tres utilisateur ----------
    colp1, colp2, colp3 = st.columns(3)
    with colp1:
        type_modele = st.selectbox(
            "Choisir un mod√®le",
            ["R√©gression lin√©aire", "R√©gression polynomiale", "For√™t al√©atoire",
             "KNN r√©gression", "R√©seau de neurones (ANN)"]
        )
    with colp2:
        taille_test = st.slider("Taille test (%)", 10, 40, 20, step=5)
    with colp3:
        graine = st.number_input("Graine al√©atoire", value=42, step=1)

    # --------- Pr√©paration X / y ----------
    # Chargeons les caract√©ristiques pour pr√©dire l'incidence (%)
    X = donnees_nettoyees[["annee", "regions_districts", "villes_communes", "maladie"]]  # S√©lectionnons les features
    y = donnees_nettoyees["incidence_population_pct"]                                     # D√©finissons la cible

    # D√©finissons colonnes num√©riques et cat√©gorielles
    colonnes_numeriques = ["annee"]                                                       # Une seule num√©rique
    colonnes_categorielles = ["regions_districts", "villes_communes", "maladie"]          # Trois cat√©gorielles

    # Construisons le pr√©processeur (standardisation + one-hot)
    preprocesseur = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), colonnes_numeriques),                               # Standardisons les num√©riques
            ("cat", OneHotEncoder(handle_unknown="ignore"), colonnes_categorielles)       # Encodons en one-hot les cat√©gorielles
        ]
    )

    # D√©composons les donn√©es en apprentissage/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=taille_test/100, random_state=int(graine)
    )

    # --------- S√©lection/Construction du pipeline mod√®le ----------
    if type_modele == "R√©gression lin√©aire":
        pipeline = Pipeline([("prep", preprocesseur), ("mod", LinearRegression())])

    elif type_modele == "R√©gression polynomiale":
        pipeline = Pipeline([
            ("prep", preprocesseur),
            ("poly", PolynomialFeatures(degree=2, include_bias=False)),
            ("mod", LinearRegression())
        ])

    elif type_modele == "For√™t al√©atoire":
        pipeline = Pipeline([
            ("prep", preprocesseur),
            ("mod", RandomForestRegressor(n_estimators=300, random_state=int(graine)))
        ])

    elif type_modele == "KNN r√©gression":
        pipeline = Pipeline([
            ("prep", preprocesseur),
            ("mod", KNeighborsRegressor(n_neighbors=7))
        ])

    else:
        pipeline = Pipeline([
            ("prep", preprocesseur),
            ("mod", MLPRegressor(hidden_layer_sizes=(128, 64),
                                 activation="relu", solver="adam",
                                 max_iter=1000, random_state=int(graine)))
        ])

    # --------- Entra√Ænement + √âvaluation ----------
    pipeline.fit(X_train, y_train)                             # Apprenons le mod√®le sur le jeu train
    y_pred = pipeline.predict(X_test)                          # Pr√©dictions sur X_test

    # Calculons les m√©triques (RMSE conforme sklearn>=1.4 via squared=False)
    r2 = r2_score(y_test, y_pred)                              # Coefficient de d√©termination
    mae = mean_absolute_error(y_test, y_pred)                  # Erreur absolue moyenne
    rmse = mean_squared_error(y_test, y_pred, squared=False)   # Racine de l‚Äôerreur quadratique

    colm1, colm2, colm3 = st.columns(3)                        # Trois cartes de m√©triques
    colm1.metric("R¬≤", f"{r2:0.3f}")                           # Affichons le R¬≤
    colm2.metric("MAE", f"{mae:0.3f}")                         # Affichons le MAE
    colm3.metric("RMSE", f"{rmse:0.3f}")                       # Affichons le RMSE

    # Cross-validation 5-fold pour robustesse (sur l‚Äôensemble complet)
    with st.expander("Voir la validation crois√©e (5 plis)"):
        scores = cross_val_score(pipeline, X, y, cv=5, scoring="r2")  # Calculons les R¬≤ en CV
        st.write("Scores R¬≤ par pli :", [f"{s:0.3f}" for s in scores])  # Affichons les scores par pli
        st.write("R¬≤ moyen :", f"{np.mean(scores):0.3f} ¬± {np.std(scores):0.3f}")  # Moyenne et √©cart-type

    # Mettons le pipeline dans la session pour r√©utilisation dans l‚Äôonglet Pr√©diction
    st.session_state["pipeline_modele"] = pipeline                      # Stockons le pipeline entra√Æn√©
    st.success("Mod√®le entra√Æn√© et stock√© pour l‚Äôonglet ‚óª Pr√©diction.") # Indiquons la disponibilit√©

# =========================
# ‚óª PR√âDICTION
# =========================
with onglets[7]:
    st.header("Pr√©dire l‚Äôincidence (%) selon vos s√©lections")           # Titre de section

    if "pipeline_modele" not in st.session_state:                       # V√©rifions qu‚Äôun mod√®le existe
        st.warning("Veuillez d‚Äôabord entra√Æner un mod√®le dans l‚Äôonglet „ÄΩÔ∏è Mod√©lisation.")
    else:
        pipe = st.session_state["pipeline_modele"]                      # R√©cup√©rons le pipeline

        colpr1, colpr2, colpr3, colpr4 = st.columns(4)                  # Quatre crit√®res d‚Äôentr√©e
        with colpr1:
            annee_sel = st.selectbox("Ann√©e", sorted(donnees_nettoyees["annee"].dropna().astype(int).unique()))
        with colpr2:
            region_sel = st.selectbox("R√©gion/District", sorted(donnees_nettoyees["regions_districts"].dropna().unique()))
        with colpr3:
            ville_sel = st.selectbox("Ville/Commune", sorted(donnees_nettoyees["villes_communes"].dropna().unique()))
        with colpr4:
            maladie_sel = st.selectbox("Maladie", sorted(donnees_nettoyees["maladie"].dropna().unique()))

        # Construisons un DataFrame d‚Äôune ligne avec ces choix
        saisie = pd.DataFrame({
            "annee": [int(annee_sel)],
            "regions_districts": [region_sel],
            "villes_communes": [ville_sel],
            "maladie": [maladie_sel]
        })

        if st.button("üîÆ Lancer la pr√©diction"):
            y_hat = pipe.predict(saisie)[0]                             # Produisons la pr√©diction
            st.success(f"Incidence attendue : **{y_hat:.2f} %**")       # R√©sum√© clair

            # Optionnel : rapprochons de la moyenne historique locale comme rep√®re
            cond = (
                (donnees_nettoyees["annee"] == int(annee_sel)) &
                (donnees_nettoyees["regions_districts"] == region_sel) &
                (donnees_nettoyees["villes_communes"] == ville_sel) &
                (donnees_nettoyees["maladie"] == maladie_sel)
            )
            ref_local = donnees_nettoyees.loc[cond, "incidence_population_pct"].mean()
            if not np.isnan(ref_local):
                st.info(f"Moyenne observ√©e (m√™mes filtres, historique) : **{ref_local:.2f} %**")

# =========================
# üõñ SOURCE
# =========================
with onglets[8]:
    st.header("Origine des donn√©es")                                                      # Titre de section
    st.markdown("""
    - **Source ouverte** : *Incidences de maladies sur la population de 2012 √† 2015*.  
    - **Lien** : https://data.gouv.ci/datasets/incidence-de-maladies-sur-la-population-de-2012-a-2015
    """)
    st.write("Utilisez les autres onglets pour naviguer dans l‚Äôanalyse, la mod√©lisation et la pr√©diction.")
